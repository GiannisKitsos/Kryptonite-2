{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49501099",
   "metadata": {},
   "source": [
    "First, we sets up the environment by importing libraries and ensuring a folder exists to store all generated plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc594571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, log_loss\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "PLOT_DIR = \"Plots\"\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf1f32",
   "metadata": {},
   "source": [
    "Then, we split the dataset into train / validation / test subsets with stratification and normalizes all features using **StandardScaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44c31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path_x, file_path_y, test_size=0.2, val_size=0.2):\n",
    "    X = np.load(file_path_x)\n",
    "    y = np.load(file_path_y)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(test_size + val_size), random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=test_size / (test_size + val_size),\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91683c2b",
   "metadata": {},
   "source": [
    "For the theoretical analysis part, we increase the polynomial degree of the input features to test whether giving logistic regression enough nonlinear feature combinations allows it to capture the dataâ€™s underlying patterns and achieve accurate classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3489756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(43201) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running: n=10, degree=9 =====\n",
      "Train shape: (12000, 10)\n",
      "Expanded features: 92378\n",
      "Training logistic regression model...\n",
      "âœ… Model fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [16:10<00:00, 970.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Train acc=0.9838, loss=0.1193 | Val acc=0.8878, loss=0.3786 | Test acc=0.8885, loss=0.3548\n",
      "n=10, deg=9 â†’ train_acc=0.984, val_acc=0.888, test_acc=0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "possible_n_vals = [10]\n",
    "possible_e_vals = [9]\n",
    "\n",
    "def run_poly_logistic_regression(n, e):\n",
    "\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "    \n",
    "    # Shuffle and split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"\\n===== Running: n={n}, degree={e} =====\")\n",
    "    print(f\"Train shape: {X_train.shape}\")\n",
    "\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=e)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly   = poly.transform(X_val)\n",
    "    X_test_poly  = poly.transform(X_test)\n",
    "\n",
    "    features = X_train_poly.shape[-1]\n",
    "    print(f\"Expanded features: {features}\")\n",
    "\n",
    "    # Initialize and fit logistic regression\n",
    "    logreg = LogisticRegression(max_iter=50000, solver='lbfgs', C=0.5)\n",
    "    print(\"Training logistic regression model...\")\n",
    "    logreg.fit(X_train_poly, y_train)\n",
    "    print(\"âœ… Model fitted\")\n",
    "\n",
    "    # Evaluate on the training data\n",
    "    y_train_pred = logreg.predict(X_train_poly)\n",
    "    y_train_proba = logreg.predict_proba(X_train_poly)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_loss = log_loss(y_train, y_train_proba)\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = logreg.predict(X_val_poly)\n",
    "    y_val_proba = logreg.predict_proba(X_val_poly)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_loss = log_loss(y_val, y_val_proba)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_test_pred = logreg.predict(X_test_poly)\n",
    "    y_test_proba = logreg.predict_proba(X_test_poly)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_loss = log_loss(y_test, y_test_proba)\n",
    "\n",
    "    print(f\"ðŸ“Š Train acc={train_acc:.4f}, loss={train_loss:.4f} | Val acc={val_acc:.4f}, loss={val_loss:.4f} | Test acc={test_acc:.4f}, loss={test_loss:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"degree\": e,\n",
    "        \"features\": features,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_loss\": test_loss\n",
    "    }\n",
    "\n",
    "# Iterate through the values\n",
    "results = []\n",
    "for n in tqdm(possible_n_vals, desc=\"Datasets\"):\n",
    "    for e in tqdm(possible_e_vals, desc=f\"Degrees for n={n}\", leave=False):\n",
    "        res = run_poly_logistic_regression(n, e)\n",
    "        results.append(res)\n",
    "\n",
    "# Print the results\n",
    "for r in results:\n",
    "    print(f\"n={r['n']}, deg={r['degree']} â†’ train_acc={r['train_acc']:.3f}, val_acc={r['val_acc']:.3f}, test_acc={r['test_acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b5ac0",
   "metadata": {},
   "source": [
    "We move on to our own MLP.\n",
    "\n",
    "Here we define the following:\n",
    "- BASE_PARAMS defines default settings for the MLP classifier.\n",
    "- HPP_MAP overrides certain hyperparameters depending on the dataset dimensionality (n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e55e3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PARAMS = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"batch_size\": 128,\n",
    "    \"max_iter\": 1000,\n",
    "    \"early_stopping\": True,\n",
    "    \"n_iter_no_change\": 50,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "HPP_MAP = {\n",
    "    20: {\"hidden_layer_sizes\": (256, 128, 64), \"learning_rate_init\": 0.001, \"alpha\": 0.1},\n",
    "    18: {\"hidden_layer_sizes\": (128, 128, 64), \"learning_rate_init\": 0.002, \"alpha\": 0.1},\n",
    "    16: {\"hidden_layer_sizes\": (128, 64), \"learning_rate_init\": 0.001, \"alpha\": 0.1},\n",
    "    14: {\"hidden_layer_sizes\": (128, 64), \"learning_rate_init\": 0.002, \"alpha\": 0.1},\n",
    "    12: {\"hidden_layer_sizes\": (128, 64), \"learning_rate_init\": 0.002, \"alpha\": 0.1},\n",
    "    10: {\"hidden_layer_sizes\": (128, 64), \"learning_rate_init\": 0.002, \"alpha\": 0.1},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdee80b",
   "metadata": {},
   "source": [
    "Function that saves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adcf49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b9c5e",
   "metadata": {},
   "source": [
    "For each dataset (kryptonite-n), we load te data and run 5-fold cross-validation. Then, we train the MLP model and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "760397d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing kryptonite-10 =====\n",
      "Cross-validation accuracies: [0.9553125 0.964375  0.95625   0.9609375 0.96125  ]\n",
      "Mean CV accuracy: 0.9596 Â± 0.0034\n",
      "Model saved to models/features-10-hidden-(128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9603\n",
      "Test Accuracy (held-out): 0.9613\n",
      "\n",
      "===== Processing kryptonite-12 =====\n",
      "Cross-validation accuracies: [0.959375   0.95677083 0.95885417 0.95989583 0.95390625]\n",
      "Mean CV accuracy: 0.9578 Â± 0.0022\n",
      "Model saved to models/features-12-hidden-(128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9620\n",
      "Test Accuracy (held-out): 0.9556\n",
      "\n",
      "===== Processing kryptonite-14 =====\n",
      "Cross-validation accuracies: [0.95245536 0.95491071 0.95580357 0.95580357 0.95357143]\n",
      "Mean CV accuracy: 0.9545 Â± 0.0013\n",
      "Model saved to models/features-14-hidden-(128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9669\n",
      "Test Accuracy (held-out): 0.9650\n",
      "\n",
      "===== Processing kryptonite-16 =====\n",
      "Cross-validation accuracies: [0.93164062 0.9234375  0.9203125  0.93828125 0.93300781]\n",
      "Mean CV accuracy: 0.9293 Â± 0.0066\n",
      "Model saved to models/features-16-hidden-(128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9570\n",
      "Test Accuracy (held-out): 0.9428\n",
      "\n",
      "===== Processing kryptonite-18 =====\n",
      "Cross-validation accuracies: [0.92847222 0.94826389 0.94045139 0.90208333 0.91215278]\n",
      "Mean CV accuracy: 0.9263 Â± 0.0172\n",
      "Model saved to models/features-18-hidden-(128, 128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9669\n",
      "Test Accuracy (held-out): 0.9472\n",
      "\n",
      "===== Processing kryptonite-20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: [0.82203125 0.85609375 0.8446875  0.92609375 0.91796875]\n",
      "Mean CV accuracy: 0.8734 Â± 0.0413\n",
      "Model saved to models/features-20-hidden-(256, 128, 64)-alpha-0.1.joblib\n",
      "Training Accuracy: 0.9720\n",
      "Test Accuracy (held-out): 0.9505\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "cv_scores_dict = {}\n",
    "\n",
    "for n in range(10, 22, 2):\n",
    "    if n not in HPP_MAP:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n===== Processing kryptonite-{n} =====\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(\n",
    "        f\"Datasets/kryptonite-{n}-X.npy\",\n",
    "        f\"Datasets/kryptonite-{n}-y.npy\",\n",
    "        test_size=0.2, val_size=0.2\n",
    "    )\n",
    "\n",
    "    X_cv = np.vstack((X_train, X_val))\n",
    "    y_cv = np.concatenate((y_train, y_val))\n",
    "\n",
    "    hpp = BASE_PARAMS.copy()\n",
    "    hpp.update(HPP_MAP[n])\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_params = hpp.copy()\n",
    "    cv_params[\"early_stopping\"] = False\n",
    "\n",
    "    model_cv = MLPClassifier(**cv_params)\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model_cv, X_cv, y_cv, cv=kfold, scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_scores_dict[n] = cv_scores\n",
    "\n",
    "    print(f\"Cross-validation accuracies: {cv_scores}\")\n",
    "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "    # Final training\n",
    "    model = MLPClassifier(**hpp)\n",
    "    model.fit(X_cv, y_cv)\n",
    "    \n",
    "    save_model(\n",
    "    model,\n",
    "    f\"models/features-{n}-hidden-{hpp['hidden_layer_sizes']}-alpha-{hpp['alpha']}.joblib\",\n",
    "    )\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    acc_val = accuracy_score(y_val, y_val_pred)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"n\": n,\n",
    "        \"cv_mean\": cv_scores.mean(),\n",
    "        \"cv_std\": cv_scores.std(),\n",
    "        \"train_acc\": acc_train,\n",
    "        \"val_acc\": acc_val,\n",
    "        \"test_acc\": acc_test,\n",
    "        \"model\": model,\n",
    "    })\n",
    "\n",
    "    print(f\"Training Accuracy: {acc_train:.4f}\")\n",
    "    print(f\"Test Accuracy (held-out): {acc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a430f",
   "metadata": {},
   "source": [
    "Finally, we generate the following plots:\n",
    "- Accuracy vs dimensionality.\n",
    "- Comparison of test vs target accuracy.\n",
    "- learning curve (loss + validation accuracy across all datasets).\n",
    "- confusion matrix for the test set (across all datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eff26afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All training, evaluation, and plots saved in 'Plots/'\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 13,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "dims = np.array([r[\"n\"] for r in results])\n",
    "cv_means = np.array([r[\"cv_mean\"] for r in results])\n",
    "cv_stds = np.array([r[\"cv_std\"] for r in results])\n",
    "test_acc = np.array([r[\"test_acc\"] for r in results])\n",
    "target_acc = np.array([0.94, 0.93, 0.92, 0.91, 0.80, 0.75])[:len(dims)]\n",
    "\n",
    "# Plot 1: Accuracy vs Dimensionality\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(dims, cv_means, 'o-', color='steelblue', label='Mean CV Accuracy')\n",
    "plt.fill_between(dims, cv_means - cv_stds, cv_means + cv_stds,\n",
    "                 color='steelblue', alpha=0.15, label='CV Â±1 SD')\n",
    "plt.plot(dims, test_acc, 's--', color='darkorange', label='Test Accuracy')\n",
    "plt.plot(dims, target_acc, 'r:', linewidth=2, label='Target Accuracy')\n",
    "plt.xlabel(\"Input Dimensionality (n)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"accuracy_vs_dim.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Test vs Target Accuracy\n",
    "width = 0.35\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(dims - width/2, test_acc, width, label='Test Accuracy', color='cornflowerblue')\n",
    "plt.bar(dims + width/2, target_acc, width, label='Target Accuracy', color='lightcoral')\n",
    "plt.xlabel(\"Dataset Dimensionality (n)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"test_vs_target.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Per-Dataset Learning Curves and Confusion Matrices\n",
    "for r in results:\n",
    "    n = r[\"n\"]\n",
    "    model = r[\"model\"]\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_data(\n",
    "        f\"Datasets/kryptonite-{n}-X.npy\",\n",
    "        f\"Datasets/kryptonite-{n}-y.npy\",\n",
    "        test_size=0.2, val_size=0.2\n",
    "    )\n",
    "\n",
    "    # Combined Training Loss + Validation Accuracy\n",
    "    if hasattr(model, \"loss_curve_\"):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        # X-axis for training loss\n",
    "        epochs = np.arange(1, len(model.loss_curve_) + 1)\n",
    "        plt.plot(epochs, model.loss_curve_, color=\"royalblue\", linewidth=2, label=\"Training Loss\")\n",
    "\n",
    "        # X-axis for validation accuracy\n",
    "        if hasattr(model, \"validation_scores_\"):\n",
    "            val_epochs = np.arange(1, len(model.validation_scores_) + 1)\n",
    "            plt.plot(val_epochs, model.validation_scores_,\n",
    "                     color=\"darkorange\", linewidth=2, label=\"Validation Accuracy\")\n",
    "            \n",
    "        plt.xlabel(\"Epoch\", fontsize=11)\n",
    "        plt.ylabel(\"Loss / Accuracy\", fontsize=11)\n",
    "        plt.legend(frameon=True, loc=\"best\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOT_DIR, f\"learning_curve_kryptonite_{n}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues', values_format=\"d\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"cm_kryptonite_{n}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\nAll training, evaluation, and plots saved in '{PLOT_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e5bf6",
   "metadata": {},
   "source": [
    "Run the MLP on the hidden datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_path: str, output_path: str, model):\n",
    "    X = np.load(input_path)\n",
    "    predictions = model.predict(X)\n",
    "    np.save(output_path, predictions)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "os.makedirs(\"hiddenlabels\", exist_ok=True)\n",
    "for n in range(10, 22, 2):\n",
    "    input_path = f\"Datasets/hidden-kryptonite-{n}-X.npy\"\n",
    "    output_path = f\"hiddenlabels/y_predicted_{n}.npy\"\n",
    "    predict(input_path, output_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0933b",
   "metadata": {},
   "source": [
    "Run KNN model for each dataset for comparioson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d861ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing KNN for kryptonite-10 =====\n",
      "Cross-validation accuracies: [0.9588 0.9609 0.9572 0.9612 0.9622]\n",
      "Mean CV accuracy: 0.9601 Â± 0.0018\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.9613\n",
      "\n",
      "===== Processing KNN for kryptonite-12 =====\n",
      "Cross-validation accuracies: [0.8646 0.8659 0.8581 0.8568 0.8625]\n",
      "Mean CV accuracy: 0.8616 Â± 0.0036\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.9092\n",
      "\n",
      "===== Processing KNN for kryptonite-14 =====\n",
      "Cross-validation accuracies: [0.6583 0.6308 0.6549 0.6484 0.6491]\n",
      "Mean CV accuracy: 0.6483 Â± 0.0095\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.6986\n",
      "\n",
      "===== Processing KNN for kryptonite-16 =====\n",
      "Cross-validation accuracies: [0.448  0.4441 0.4373 0.452  0.4438]\n",
      "Mean CV accuracy: 0.4450 Â± 0.0049\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.4495\n",
      "\n",
      "===== Processing KNN for kryptonite-18 =====\n",
      "Cross-validation accuracies: [0.4519 0.462  0.4557 0.4608 0.4498]\n",
      "Mean CV accuracy: 0.4560 Â± 0.0048\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.4562\n",
      "\n",
      "===== Processing KNN for kryptonite-20 =====\n",
      "Cross-validation accuracies: [0.4794 0.4781 0.4791 0.4855 0.4741]\n",
      "Mean CV accuracy: 0.4792 Â± 0.0037\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy (held-out): 0.4734\n"
     ]
    }
   ],
   "source": [
    "KNN_PARAMS = {\n",
    "    \"n_neighbors\": 6,\n",
    "    \"metric\": \"euclidean\",\n",
    "    \"weights\": \"distance\"\n",
    "}\n",
    "\n",
    "knn_results = []\n",
    "knn_cv_scores_dict = {}\n",
    "\n",
    "for knn_n in range(10, 22, 2):\n",
    "    print(f\"\\n===== Processing KNN for kryptonite-{knn_n} =====\")\n",
    "\n",
    "    X_train_knn, X_val_knn, X_test_knn, y_train_knn, y_val_knn, y_test_knn = load_data(\n",
    "        f\"Datasets/kryptonite-{knn_n}-X.npy\",\n",
    "        f\"Datasets/kryptonite-{knn_n}-y.npy\",\n",
    "        test_size=0.2, val_size=0.2\n",
    "    )\n",
    "\n",
    "    # Combine train + val for CV\n",
    "    X_cv_knn = np.vstack((X_train_knn, X_val_knn))\n",
    "    y_cv_knn = np.concatenate((y_train_knn, y_val_knn))\n",
    "\n",
    "    # Standardize\n",
    "    scaler_knn = StandardScaler()\n",
    "    X_train_knn = scaler_knn.fit_transform(X_train_knn)\n",
    "    X_val_knn = scaler_knn.transform(X_val_knn)\n",
    "    X_test_knn = scaler_knn.transform(X_test_knn)\n",
    "    X_cv_knn = scaler_knn.fit_transform(X_cv_knn)\n",
    "\n",
    "    # Cross-validation\n",
    "    model_cv_knn = KNeighborsClassifier(**KNN_PARAMS)\n",
    "    kfold_knn = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores_knn = cross_val_score(model_cv_knn, X_cv_knn, y_cv_knn, cv=kfold_knn, scoring=\"accuracy\", n_jobs=-1)\n",
    "    knn_cv_scores_dict[knn_n] = cv_scores_knn\n",
    "\n",
    "    print(f\"Cross-validation accuracies: {np.round(cv_scores_knn, 4)}\")\n",
    "    print(f\"Mean CV accuracy: {cv_scores_knn.mean():.4f} Â± {cv_scores_knn.std():.4f}\")\n",
    "\n",
    "    # Final training on full train+val\n",
    "    model_knn = KNeighborsClassifier(**KNN_PARAMS)\n",
    "    model_knn.fit(X_cv_knn, y_cv_knn)\n",
    "\n",
    "    # Evaluate\n",
    "    y_train_pred_knn = model_knn.predict(X_train_knn)\n",
    "    y_val_pred_knn = model_knn.predict(X_val_knn)\n",
    "    y_test_pred_knn = model_knn.predict(X_test_knn)\n",
    "\n",
    "    acc_train_knn = accuracy_score(y_train_knn, y_train_pred_knn)\n",
    "    acc_val_knn = accuracy_score(y_val_knn, y_val_pred_knn)\n",
    "    acc_test_knn = accuracy_score(y_test_knn, y_test_pred_knn)\n",
    "\n",
    "    knn_results.append({\n",
    "        \"n\": knn_n,\n",
    "        \"cv_mean\": cv_scores_knn.mean(),\n",
    "        \"cv_std\": cv_scores_knn.std(),\n",
    "        \"train_acc\": acc_train_knn,\n",
    "        \"val_acc\": acc_val_knn,\n",
    "        \"test_acc\": acc_test_knn,\n",
    "        \"model\": model_knn,\n",
    "    })\n",
    "\n",
    "    print(f\"Training Accuracy: {acc_train_knn:.4f}\")\n",
    "    print(f\"Test Accuracy (held-out): {acc_test_knn:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
